<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Looking here or there? Gaze Following in 360-Degree Images">
    <meta name="author" content="Yunhao Li,
                                Wei Shen,
                                Zhongpai Gao,
                                Yucheng Zhu, Guangtao Zhai, Guodong Guo">

    <title>Looking here or there? Gaze Following in 360-Degree Images</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Looking here or there? Gaze Following in 360-Degree Images</h2>
    <h3>ICCV 2021</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p class="authors">
        Yunhao Li</a>,
        Wei Shen</a>,
        Zhongpai Gao</a>,
        Yucheng Zhu</a>,</br>
        Guangtao Zhai</a>,
        Guodong Guo</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
<!--         <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a> -->
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/Q2fLWGBeaiI" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
	<div class="col-sm-3">
            <h5>Siren</h5>
                <img src="img/audio/siren_bach.png" style="width:100%">
                <audio
                        controls
                        src="img/audio/siren_bach.wav"
                        class="media-left"
                        style="width:100%">
                    Your browser does not support the
                    <code>audio</code> element.
                </audio>
        </div>
        <hr>
        <p>
<!--             can adding some words -->
        </p>
    </div>

    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
	    Gaze following, i.e., detecting the gaze target of a human subject, in 2D images has become an active topic in computer vision. However, it usually 
	    suffers from the out of frame issue due to the limited field-of-view (FoV) of 2D images. In this paper, we introduce a novel task, gaze following in 
	    360-degree images which provide an omnidirectional FoV and can alleviate the out of frame issue. We collect the first dataset, 
            "GazeFollow360", for this task, containing around 10,000 360-degree 
	    images with complex gaze behaviors under various scenes. Existing 2D gaze following methods suffer from performance degradation in 360-degree images since 
	    they may use the assumption that a gaze target is in the 2D gaze sight line. However, this assumption is no longer true for long-distance gaze behaviors in 
	    360-degree images, due to the distortion brought by sphere-to-plane projection. To address this challenge, 
	    we propose a 3D sight line guided dual-pathway framework, to detect the gaze target within a local region (here) and from a distant region (there), parallelly. 
	    Specifically, the local region is obtained as a 2D cone-shaped field along the 2D projection of the sight line starting at the human subjectâ€™s head position, 
	    and the distant region is obtained by searching along the sight line in 3D sphere space. Finally, the location of the gaze target is determined by fusing 
	    the estimations from both the local region and the distant region. Experimental results show that our method achieves significant improvements over 
	    previous 2D gaze following methods on our GazeFollow360 dataset.
        </p>
    </div>

    <div class="section">
        <h2>GazeFollow360 Dataset</h2>
        <hr>
        <p>
	    (Coming Soon!) The dataset is available for free only for research purposes. You download the dataset from here. We greatly welcome emails 
	    about questions or suggestions. Please email to lyhsjtu@sjtu.edu.cn. Please cite this paper if you use the dataset:
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/image_convergence_15s_label.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
        <div class="row align-items-center">
			<div class="col justify-content-center text-center">
                <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/psnr_image_convergence_15s.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>

    <div class="section">
        <h2>Paper and Supplementary Material</h2>
        <hr>
        <p>
            A Siren with a single, time-coordinate input and scalar output may parameterize audio signals. Siren is the
            only network architecture that succeeds in reproducing the audio signal, both for music and human voice.
        </p>
        <div class="row justify-content-left">
            <div class="col-sm-3">
                <h5>Ground truth</h5>
                <img src="img/audio/gt_bach.png" style="width:100%">
                <audio
                        controls
                        src="img/audio/gt_bach.wav"
                        class="media-left"
                        style="width:100%">
                    Your browser does not support the
                    <code>audio</code> element.
                </audio>
            </div>
            <div class="col-sm-3">
                <h5>ReLU MLP</h5>
                <img src="img/audio/relu_bach.png" style="width:100%">
                <audio
                        controls
                        src="img/audio/relu_bach.wav"
                        class="media-left"
                        style="width:100%">
                    Your browser does not support the
                    <code>audio</code> element.
                </audio>
            </div>
            <div class="col-sm-3">
                <h5>ReLU w/ Pos. Enc.</h5>
                <img src="img/audio/relu_pe_bach.png" style="width:100%">
                <audio
                        controls
                        src="img/audio/relu_pe_bach.wav"
                        class="media-left"
                        style="width:100%">
                    Your browser does not support the
                    <code>audio</code> element.
                </audio>
            </div>
            <div class="col-sm-3">
                <h5>Siren</h5>
                <img src="img/audio/siren_bach.png" style="width:100%">
                <audio
                        controls
                        src="img/audio/siren_bach.wav"
                        class="media-left"
                        style="width:100%">
                    Your browser does not support the
                    <code>audio</code> element.
                </audio>
            </div>

            <div class="row justify-content-left">
                <div class="col-sm-3">
<!--                    <h5>Ground truth</h5>-->
                    <img src="img/audio/gt_counting.png" style="width:100%">
                    <audio
                            controls
                            src="img/audio/gt_counting.wav"
                            class="media-left"
                            style="width:100%">
                        Your browser does not support the
                        <code>audio</code> element.
                    </audio>
                </div>
                <div class="col-sm-3">
<!--                    <h5>ReLU MLP</h5>-->
                    <img src="img/audio/relu_counting.png" style="width:100%">
                    <audio
                            controls
                            src="img/audio/relu_counting.wav"
                            class="media-left"
                            style="width:100%">
                        Your browser does not support the
                        <code>audio</code> element.
                    </audio>
                </div>
                <div class="col-sm-3">
<!--                    <h5>ReLU P.E.</h5>-->
                    <img src="img/audio/relu_pe_counting.png" style="width:100%">
                    <audio
                            controls
                            src="img/audio/relu_pe_counting.wav"
                            class="media-left"
                            style="width:100%">
                        Your browser does not support the
                        <code>audio</code> element.
                    </audio>
                </div>
                <div class="col-sm-3">
<!--                    <h5>Siren</h5>-->
                    <img src="img/audio/siren_counting.png" style="width:100%">
                    <audio
                            controls
                            src="img/audio/siren_counting.wav"
                            class="media-left"
                            style="width:100%">
                        Your browser does not support the
                        <code>audio</code> element.
                    </audio>
                </div>
        </div>
    </div>





<!--     <div class="section">
        <h2>Related Projects</h2>
        <hr>
        <p>
            Check out our related projects on the topic of implicit neural representations! <br>
        </p>
        <div class='row vspace-top'>
            <div class="col-sm-3">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/metasdf_steps_comp.mp4" type="video/mp4">
                </video>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="http://vsitzmann.github.io/metasdf/">MetaSDF: Meta-learning Signed Distance Functions</a>
                </div>
                <div>
                    We identify a key relationship between generalization across implicit neural representations and meta-
                    learning, and propose to leverage gradient-based meta-learning for learning priors over deep signed distance
                    functions. This allows us to reconstruct SDFs an order of magnitude faster than the auto-decoder framework,
                    with no loss in performance!
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/SRNs.gif' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="http://vsitzmann.github.io/srns/">Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations</a>

                </div>
                <div>
                    A continuous, 3D-structure-aware neural scene representation that encodes both geometry and appearance,
                    supervised only in 2D via a neural renderer, and generalizes for 3D reconstruction from a single posed 2D image.
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/srn_seg_repimage.jpg' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="https://www.computationalimaging.org/publications/semantic-srn/">Inferring Semantic Information with 3D Neural Scene Representations
                    </a>
                </div>
                <div>
                    We demonstrate that the features learned by neural implicit scene representations are useful for downstream
                    tasks, such as semantic segmentation, and propose a model that can learn to perform continuous 3D
                    semantic segmentation on a class of objects (such as chairs) given only a single, 2D (!) semantic label map!
                </div>
            </div>
        </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2006.09661"
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{sitzmann2019siren,
                author = {Sitzmann, Vincent
                          and Martel, Julien N.P.
                          and Bergman, Alexander W.
                          and Lindell, David B.
                          and Wetzstein, Gordon},
                title = {Implicit Neural Representations
                          with Periodic Activation Functions},
                booktitle = {Proc. NeurIPS},
                year={2020}
            }
        </div>
    </div>
 -->
	    
	    
    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
<!--             @inproceedings{sitzmann2019siren,
                author = {Sitzmann, Vincent
                          and Martel, Julien N.P.
                          and Bergman, Alexander W.
                          and Lindell, David B.
                          and Wetzstein, Gordon},
                title = {Implicit Neural Representations
                          with Periodic Activation Functions},
                booktitle = {Proc. NeurIPS},
                year={2020} -->
            }
        </div>
    </div>
    <hr>

    <footer>
        <p>Send feedback and questions to lyhsjtu@sjtu.edu.cn</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
